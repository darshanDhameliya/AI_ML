{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tf2onnx --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"keras-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import tf2onnx\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3064321 entries, 0 to 3064320\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   jobTitle       object\n",
      " 1   jobLevel       object\n",
      " 2   jobDepartment  object\n",
      " 3   jobFunction    object\n",
      "dtypes: object(4)\n",
      "memory usage: 93.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('/home/darshan/Documents/Master_Title_Department.csv')\n",
    "\n",
    "data.info();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing values in the jobTitle column\n",
    "data = data.dropna(subset=['jobTitle'])\n",
    "\n",
    "# Ensure jobTitle is of type string\n",
    "data['jobTitle'] = data['jobTitle'].astype(str)\n",
    "\n",
    "# Preprocess the data\n",
    "label_encoder_level = LabelEncoder()\n",
    "label_encoder_department = LabelEncoder()\n",
    "\n",
    "data['jobLevel'] = label_encoder_level.fit_transform(data['jobLevel'])\n",
    "data['jobDepartment'] = label_encoder_department.fit_transform(data['jobDepartment'])\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train_level, y_test_level, y_train_department, y_test_department = train_test_split(\n",
    "    data['jobTitle'], data['jobLevel'], data['jobDepartment'], test_size=0.01, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the job titles\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = max(len(seq) for seq in x_train_seq)\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=max_length)\n",
    "x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (max_length,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darshan/dev/DR/training/AI_ML/ai_ml_venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-12-12 16:23:35.707515: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 16:23:36.068218: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 266963576 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8171/94803\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35:42\u001b[0m 25ms/step - accuracy: 0.8826 - loss: 0.3673"
     ]
    }
   ],
   "source": [
    "# Define the model for jobLevel\n",
    "level_inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "level_hidden = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=56, input_length=max_length)(level_inputs)\n",
    "level_hidden = tf.keras.layers.GlobalAveragePooling1D()(level_hidden)\n",
    "level_hidden = tf.keras.layers.Dense(56, activation='relu')(level_hidden)\n",
    "level_outputs = tf.keras.layers.Dense(len(label_encoder_level.classes_), activation='softmax')(level_hidden)\n",
    "\n",
    "# Create the model\n",
    "model_level = tf.keras.Model(inputs=level_inputs, outputs=level_outputs)\n",
    "\n",
    "# compile & fit\n",
    "model_level.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_level.fit(x_train_padded, y_train_level, epochs=2, validation_data=(x_test_padded, y_test_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for jobDepartment\n",
    "department_inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "department_hidden = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=56, input_length=max_length)(department_inputs)\n",
    "department_hidden = tf.keras.layers.GlobalAveragePooling1D()(department_hidden)\n",
    "department_hidden = tf.keras.layers.Dense(56, activation='relu')(department_hidden)\n",
    "department_outputs = tf.keras.layers.Dense(len(label_encoder_department.classes_), activation='softmax')(department_hidden)\n",
    "\n",
    "# Create the model\n",
    "model_department = tf.keras.Model(inputs=department_inputs, outputs=department_outputs)\n",
    "\n",
    "model_department.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_department.fit(x_train_padded, y_train_department, epochs=1, validation_data=(x_test_padded, y_test_department))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:21:48.387319Z",
     "iopub.status.busy": "2024-11-04T10:21:48.386538Z",
     "iopub.status.idle": "2024-11-04T10:21:51.525636Z",
     "shell.execute_reply": "2024-11-04T10:21:51.524507Z",
     "shell.execute_reply.started": "2024-11-04T10:21:48.387275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the probabilities for the test set\n",
    "y_pred_prob_level = model_level.predict(x_test_padded)\n",
    "y_pred_prob_department = model_department.predict(x_test_padded)\n",
    "\n",
    "# Convert the probabilities to class labels\n",
    "y_pred_level = np.argmax(y_pred_prob_level, axis=1)\n",
    "y_pred_department = np.argmax(y_pred_prob_department, axis=1)\n",
    "\n",
    "# Calculate the accuracy for each model\n",
    "accuracy_level = accuracy_score(y_test_level, y_pred_level)\n",
    "accuracy_department = accuracy_score(y_test_department, y_pred_department)\n",
    "\n",
    "# Calculate the overall accuracy\n",
    "overall_accuracy = (accuracy_level + accuracy_department) / 2\n",
    "\n",
    "print(f'Job Level Accuracy: {accuracy_level}')\n",
    "print(f'Job Department Accuracy: {accuracy_department}')\n",
    "print(f'Overall Accuracy: {overall_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a function to build the model with tunable parameters\n",
    "def build_model(hp):\n",
    "    level_inputs_2 = tf.keras.layers.Input(shape=input_shape)\n",
    "    level_hidden_2 = tf.keras.layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1, \n",
    "        output_dim=hp.Int('embedding_dim', min_value=8, max_value=64, step=8),\n",
    "        input_length=max_length)(level_inputs_2)\n",
    "    level_hidden_2 = tf.keras.layers.GlobalAveragePooling1D()(level_hidden_2)\n",
    "    level_hidden_2 = tf.keras.layers.Dense(\n",
    "        hp.Int('units', min_value=8, max_value=64, step=8), \n",
    "        activation='relu')(level_hidden_2)\n",
    "    level_outputs_2 = tf.keras.layers.Dense(len(label_encoder_level.classes_), activation='softmax')(level_hidden_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=level_inputs_2, outputs=level_outputs_2)\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Set up Keras Tuner to search for best hyperparameters\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of different combinations to try\n",
    "    executions_per_trial=1,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='job_level_model')\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(x_train_padded, y_train_level, epochs=2, validation_data=(x_test_padded, y_test_level))\n",
    "\n",
    "# Get the best model and retrain with more epochs\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "model_level_2 = tuner.hypermodel.build(best_hp)\n",
    "early_stopping_2 = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model_level_2.fit(x_train_padded, y_train_level, epochs=2, batch_size=best_hp.get('batch_size'), \n",
    "                  validation_data=(x_test_padded, y_test_level), callbacks=[early_stopping_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities for the test set\n",
    "y_pred_prob_level = model_level.predict(x_test_padded)\n",
    "\n",
    "# Convert the probabilities to class labels\n",
    "y_pred_level = np.argmax(y_pred_prob_level, axis=1)\n",
    "\n",
    "# Calculate the accuracy for each model\n",
    "accuracy_level = accuracy_score(y_test_level, y_pred_level)\n",
    "\n",
    "print(f'Job Level Accuracy: {accuracy_level}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and label encoders\n",
    "\n",
    "with open('/kaggle/working/tokenizer.json', 'w') as f:\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "    \n",
    "with open('/kaggle/working/tokenizer_min.json', 'w') as f:\n",
    "    tokenizer_min = {\"word_index\" : tokenizer.word_index, \"filters\" : tokenizer.filters}\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "\n",
    "with open('/kaggle/working/label_encoder_level.json', 'w') as f:\n",
    "    json.dump(label_encoder_level.classes_.tolist(), f)\n",
    "\n",
    "with open('/kaggle/working/label_encoder_department.json', 'w') as f:\n",
    "    json.dump(label_encoder_department.classes_.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in h5 format\n",
    "model_department.save('/kaggle/working/department.h5')\n",
    "model_level.save('/kaggle/working/level.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in onnx format\n",
    "\n",
    "# as both have same input sigmature\n",
    "input_signature = [tf.TensorSpec(model_level.inputs[0].shape, model_level.inputs[0].dtype)]\n",
    "\n",
    "onnx_model_department, _ = tf2onnx.convert.from_keras(model_department, input_signature, opset=13)\n",
    "onnx.save(onnx_model_department, 'department.onnx')\n",
    "\n",
    "onnx_mode_level, _ = tf2onnx.convert.from_keras(model_level, input_signature, opset=13)\n",
    "onnx.save(onnx_mode_level, 'level.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input job title\n",
    "def preprocess_input(job_title, tokenizer, max_length):\n",
    "    # Tokenize the input job title\n",
    "    seq = tokenizer.texts_to_sequences([job_title])\n",
    "    print(seq)\n",
    "    # Pad the sequence\n",
    "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_length)\n",
    "    print(padded_seq)\n",
    "    return padded_seq\n",
    "\n",
    "# Example job title to predict\n",
    "job_title = \"CFO\"\n",
    "\n",
    "# Preprocess the input job title\n",
    "input_seq = preprocess_input(job_title, tokenizer, max_length)\n",
    "input_seq"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5937752,
     "sourceId": 9708170,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 148319,
     "modelInstanceId": 125337,
     "sourceId": 155584,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
